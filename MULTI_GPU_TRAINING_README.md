# Multi-GPU Training Automation for ICICLE Benchmark

This system automates training across 4 GPUs, with each GPU handling one dataset through 4 different training settings.

## ğŸ¯ Overview

- **4 GPUs** running in parallel
- **4 datasets** (one per GPU)
- **4 training settings** per dataset (16 total trainings)
- **Sequential execution** within each GPU
- **Automatic GPU memory management** between trainings

## ğŸ“‹ Training Settings (in order)

Each GPU will execute these 4 training settings sequentially:

1. **LoRA BSM (bottleneck=8)**: `--c <dataset>_accu_bsm.yaml --lora_bottleneck 8`
2. **LoRA CE (bottleneck=8)**: `--c <dataset>_accu_ce.yaml --lora_bottleneck 8`  
3. **Full CE**: `--c <dataset>_accu_ce.yaml --full`
4. **Full BSM**: `--c <dataset>_accu_bsm.yaml --full`

## ğŸš€ Quick Start

### 1. Choose 4 Datasets

First, check available datasets:
```bash
ls config/pipeline/
```

Example datasets:
- `APN_K024`
- `APN_K034_new`
- `ENO_B06`
- `serengeti_C02`

### 2. Test Configuration (Recommended)

Test your configuration without running actual training:
```bash
./test_multi_gpu_config.sh APN_K024 APN_K034_new ENO_B06 serengeti_C02
```

### 3. Run Training

Launch the multi-GPU training:
```bash
./launch_multi_gpu_training.sh APN_K024 APN_K034_new ENO_B06 serengeti_C02
```

## ğŸ“ File Structure

```
ICICLE-Benchmark/
â”œâ”€â”€ multi_gpu_training.py          # Main automation script
â”œâ”€â”€ launch_multi_gpu_training.sh   # Easy launcher script
â”œâ”€â”€ test_multi_gpu_config.sh       # Configuration validator
â”œâ”€â”€ MULTI_GPU_TRAINING_README.md   # This file
â””â”€â”€ logs/                           # Training logs
    â”œâ”€â”€ gpu_0_<dataset>/           # Logs for GPU 0
    â”œâ”€â”€ gpu_1_<dataset>/           # Logs for GPU 1
    â”œâ”€â”€ gpu_2_<dataset>/           # Logs for GPU 2
    â””â”€â”€ gpu_3_<dataset>/           # Logs for GPU 3
```

## ğŸ”§ Advanced Usage

### Manual Python Execution

You can also run the Python script directly:
```bash
python multi_gpu_training.py \
    --datasets APN_K024 APN_K034_new ENO_B06 serengeti_C02 \
    --config-root /fs/scratch/PAS2099/camera-trap-final/configs \
    --workspace /fs/ess/PAS2099/sooyoung/ICICLE-Benchmark
```

### Options

- `--datasets`: Four dataset names (required)
- `--config-root`: Path to configuration files (default: `/fs/scratch/PAS2099/camera-trap-final/configs`)
- `--workspace`: Workspace path (default: `/fs/ess/PAS2099/sooyoung/ICICLE-Benchmark`)
- `--dry-run`: Validate configuration without running training

## ğŸ“Š Monitoring

### Real-time Monitoring

Monitor training progress in real-time:
```bash
# Watch main log
tail -f multi_gpu_training.log

# Watch specific GPU logs
tail -f logs/gpu_0_<dataset>/training_*.out
```

### Check GPU Usage

```bash
# Monitor GPU memory and usage
watch -n 1 nvidia-smi

# Check specific GPU
nvidia-smi -i 0,1,2,3
```

## ğŸ› ï¸ Features

### Automatic GPU Memory Management
- Clears GPU memory between trainings
- Prevents memory leaks and CUDA out-of-memory errors
- Safe 5-second wait between memory clear and next training

### Comprehensive Logging
- Main log: `multi_gpu_training.log`
- Per-GPU logs: `logs/gpu_<id>_<dataset>/`
- Separate stdout/stderr files for each training
- Timestamped log files

### Error Handling
- Continues with remaining trainings if one fails
- Graceful shutdown on Ctrl+C
- Detailed error reporting
- Training summary with success/failure counts

### Process Management
- Each GPU runs in separate thread
- Proper process cleanup on interruption
- PID tracking for running processes
- Timeout handling for GPU memory clearing

## ğŸ“ˆ Training Execution Flow

```
GPU 0 (Dataset A)    GPU 1 (Dataset B)    GPU 2 (Dataset C)    GPU 3 (Dataset D)
     â”‚                      â”‚                      â”‚                      â”‚
     â”œâ”€ LoRA BSM (8)        â”œâ”€ LoRA BSM (8)        â”œâ”€ LoRA BSM (8)        â”œâ”€ LoRA BSM (8)
     â”œâ”€ Clear Memory        â”œâ”€ Clear Memory        â”œâ”€ Clear Memory        â”œâ”€ Clear Memory
     â”œâ”€ LoRA CE (8)         â”œâ”€ LoRA CE (8)         â”œâ”€ LoRA CE (8)         â”œâ”€ LoRA CE (8)
     â”œâ”€ Clear Memory        â”œâ”€ Clear Memory        â”œâ”€ Clear Memory        â”œâ”€ Clear Memory
     â”œâ”€ Full CE             â”œâ”€ Full CE             â”œâ”€ Full CE             â”œâ”€ Full CE
     â”œâ”€ Clear Memory        â”œâ”€ Clear Memory        â”œâ”€ Clear Memory        â”œâ”€ Clear Memory
     â””â”€ Full BSM            â””â”€ Full BSM            â””â”€ Full BSM            â””â”€ Full BSM
```

## âš ï¸ Important Notes

### Configuration Requirements
- Config files must exist at: `/fs/scratch/PAS2099/camera-trap-final/configs/<dataset>/`
- Required files per dataset:
  - `<dataset>_accu_bsm.yaml`
  - `<dataset>_accu_ce.yaml`

### GPU Requirements
- 4 GPUs must be available (0, 1, 2, 3)
- Sufficient VRAM for your model and batch size
- CUDA-capable GPUs

### Workspace Requirements
- `run_pipeline.py` must be in the workspace root
- Write permissions for log directory creation
- Sufficient disk space for model checkpoints and logs

## ğŸ” Troubleshooting

### Common Issues

**Config file not found:**
```bash
# Check if config files exist
ls /fs/scratch/PAS2099/camera-trap-final/configs/<dataset>/
```

**GPU memory issues:**
```bash
# Check GPU memory usage
nvidia-smi

# If needed, manually clear GPU memory
python -c "import torch; torch.cuda.empty_cache()"
```

**Permission issues:**
```bash
# Make scripts executable
chmod +x launch_multi_gpu_training.sh
chmod +x test_multi_gpu_config.sh
```

### Stopping Training

- **Graceful stop**: Press `Ctrl+C` once and wait
- **Force stop**: Press `Ctrl+C` multiple times
- **Kill processes**: `pkill -f run_pipeline.py`

## ğŸ“ Example Output

```
[GPU-0][APN_K024] Starting training 1/4: LoRA BSM (bottleneck=8)
[GPU-1][APN_K034_new] Starting training 1/4: LoRA BSM (bottleneck=8)
[GPU-2][ENO_B06] Starting training 1/4: LoRA BSM (bottleneck=8)
[GPU-3][serengeti_C02] Starting training 1/4: LoRA BSM (bottleneck=8)
...
[GPU-0][APN_K024] Training 1 completed successfully!
[GPU-0][APN_K024] Clearing GPU memory...
[GPU-0][APN_K024] Starting training 2/4: LoRA CE (bottleneck=8)
...
```

## ğŸ‰ Success Indicators

- All 16 trainings complete without errors
- Log files created for each training
- Model checkpoints saved in respective directories
- Final summary shows 16 completed, 0 failed trainings

## ğŸ“ Support

If you encounter issues:
1. Check the main log: `multi_gpu_training.log`
2. Check GPU-specific logs: `logs/gpu_*/`
3. Verify configuration with dry-run mode
4. Ensure all dependencies are installed
5. Check GPU availability and memory
