/users/PAS2099/mino/miniconda3/envs/ICICLE/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
2025-07-12 15:53:19,245 - root - INFO - Saving to /fs/scratch/PAS2099/mino/ICICLE/log_auto/pipeline/serengeti_serengeti_F11/upper_bound/2025-07-12-15-53-11/bioclip2/full_text_head/2025-07-12-15-53-19/log. 
wandb: Currently logged in as: minodaze (minodaze-the-ohio-state-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.20.1
wandb: Run data is saved locally in /users/PAS2099/mino/ICICLE/wandb/run-20250712_155319-9eqtcgl1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serengeti_serengeti_F11 | upper_bound | 2025-07-12-15-53-11 | bioclip2 | full_text_head | 2025-07-12-15-53-19
wandb: ‚≠êÔ∏è View project at https://wandb.ai/minodaze-the-ohio-state-university/ICICLE-Benchmark
wandb: üöÄ View run at https://wandb.ai/minodaze-the-ohio-state-university/ICICLE-Benchmark/runs/9eqtcgl1
2025-07-12 15:53:20,603 - root - INFO - wandb logging is enabled.
2025-07-12 15:53:20,604 - root - INFO - {'accu_eval': False,
 'adapter_bottleneck': 64,
 'adapter_init': 'lora_kaiming',
 'adapter_scaler': 0.1,
 'al_config': {'method': 'none'},
 'attention_index': None,
 'attention_type': 'full',
 'bitfit': False,
 'block_index': None,
 'c': '/fs/scratch/PAS2099/mino/ICICLE/configs/generated_common/serengeti_serengeti_F11_upper_bound.yaml',
 'cl_config': {'method': 'none'},
 'common_config': {'model': 'bioclip2', 'train_data_config_path': '/fs/scratch/PAS2099/camera-trap-benchmark/serengeti/serengeti_F11/30/train.json', 'eval_data_config_path': '/fs/scratch/PAS2099/camera-trap-benchmark/serengeti/serengeti_F11/30/test.json', 'all_data_config_path': '/fs/scratch/PAS2099/camera-trap-benchmark/serengeti/serengeti_F11/30/train-all.json', 'train_batch_size': 32, 'eval_batch_size': 512, 'optimizer_name': 'AdamW', 'optimizer_params': {'lr': 2.5e-05, 'weight_decay': 0.0001}, 'chop_head': False, 'scheduler': 'CosineAnnealingLR', 'scheduler_params': {'T_max': 30, 'eta_min': 2.5e-06}},
 'convpass_bottleneck': 8,
 'convpass_init': 'lora_xavier',
 'convpass_scaler': 10,
 'convpass_xavier_init': False,
 'debug': False,
 'device': 'cuda',
 'difffit': False,
 'drop_path_rate': 0.0,
 'eval_only': False,
 'eval_per_epoch': False,
 'fact_dim': 8,
 'fact_scaler': 1.0,
 'fact_type': None,
 'ft_attn_ln': 'before',
 'ft_attn_mode': 'parallel',
 'ft_attn_module': None,
 'ft_mlp_ln': 'before',
 'ft_mlp_mode': 'parallel',
 'ft_mlp_module': None,
 'full': True,
 'generalization_test': 'a',
 'gpu_id': None,
 'gpu_memory_monitor': False,
 'interpolation_alpha': 0.5,
 'interpolation_head': False,
 'interpolation_model': False,
 'is_save': False,
 'label_type': 'common',
 'ln': False,
 'log_path': '/fs/scratch/PAS2099/mino/ICICLE/log_auto/pipeline/serengeti_serengeti_F11/upper_bound/2025-07-12-15-53-11/',
 'lora_bottleneck': 0,
 'merge_factor': 1,
 'mlp_index': None,
 'mlp_type': 'full',
 'module_name': 'upper_bound',
 'no_gpu_monitor_colors': False,
 'ood_config': {'method': 'none'},
 'pretrain_config': {'pretrain': True, 'pretrain_data_config_path': '/fs/scratch/PAS2099/camera-trap-benchmark/serengeti/serengeti_F11/30/train-all.json', 'epochs': 60, 'loss_type': 'ce'},
 'pretrained_weights': 'bioclip2',
 'repadapter_bottleneck': 8,
 'repadapter_group': 2,
 'repadapter_init': 'lora_xavier',
 'repadapter_scaler': 1,
 'save_dir': '/fs/scratch/PAS2099/mino/ICICLE/log_auto/pipeline/serengeti_serengeti_F11/upper_bound/2025-07-12-15-53-11/bioclip2/full_text_head/2025-07-12-15-53-19/log',
 'seed': 9527,
 'ssf': False,
 'text': 'head',
 'text_template': 'openai',
 'vpt_dropout': 0.1,
 'vpt_layer': None,
 'vpt_mode': None,
 'vpt_num': 10,
 'vqt_dropout': 0.1,
 'vqt_num': 0,
 'wandb': True}
2025-07-12 15:53:20,609 - root - INFO - Using Bioclip-2 model. 
2025-07-12 15:53:20,609 - root - INFO - Loaded ViT-L-14 model config.
2025-07-12 15:53:22,521 - root - INFO - Loading pretrained ViT-L-14 weights (pretrained_weights/bioclip-2/open_clip_pytorch_model.bin).
2025-07-12 15:53:28,154 - root - INFO - Pretraining classifier... 
2025-07-12 15:53:28,169 - root - INFO - Gathering samples from checkpoints: ['ckp_-1', 'ckp_1']
2025-07-12 15:53:28,198 - root - INFO - Checkpoint ckp_1 not found
2025-07-12 15:53:28,199 - root - INFO - Subset length: 1501
2025-07-12 15:53:28,199 - root - INFO - Pretrain dataset size: 1501. 
2025-07-12 15:53:47,747 - root - INFO - Epoch 0, loss: 2.1268, acc: 0.7875, lr: 0.00002500. 
2025-07-12 15:54:07,230 - root - INFO - Epoch 1, loss: 2.0225, acc: 0.9081, lr: 0.00002494. 
2025-07-12 15:54:26,555 - root - INFO - Epoch 2, loss: 1.9801, acc: 0.9360, lr: 0.00002475. 
2025-07-12 15:54:45,924 - root - INFO - Epoch 3, loss: 1.9466, acc: 0.9447, lr: 0.00002445. 
2025-07-12 15:55:05,308 - root - INFO - Epoch 4, loss: 1.9160, acc: 0.9547, lr: 0.00002403. 
2025-07-12 15:55:24,619 - root - INFO - Epoch 5, loss: 1.8792, acc: 0.9707, lr: 0.00002349. 
2025-07-12 15:55:43,904 - root - INFO - Epoch 6, loss: 1.8655, acc: 0.9447, lr: 0.00002285. 
2025-07-12 15:56:03,243 - root - INFO - Epoch 7, loss: 1.8245, acc: 0.9700, lr: 0.00002211. 
2025-07-12 15:56:22,612 - root - INFO - Epoch 8, loss: 1.8018, acc: 0.9594, lr: 0.00002128. 
2025-07-12 15:56:41,958 - root - INFO - Epoch 9, loss: 1.7693, acc: 0.9740, lr: 0.00002036. 
2025-07-12 15:57:01,365 - root - INFO - Epoch 10, loss: 1.7424, acc: 0.9793, lr: 0.00001937. 
2025-07-12 15:57:20,685 - root - INFO - Epoch 11, loss: 1.7146, acc: 0.9853, lr: 0.00001833. 
2025-07-12 15:57:40,009 - root - INFO - Epoch 12, loss: 1.6904, acc: 0.9893, lr: 0.00001723. 
2025-07-12 15:57:59,365 - root - INFO - Epoch 13, loss: 1.6697, acc: 0.9887, lr: 0.00001609. 
2025-07-12 15:58:18,665 - root - INFO - Epoch 14, loss: 1.6511, acc: 0.9893, lr: 0.00001493. 
2025-07-12 15:58:38,120 - root - INFO - Epoch 15, loss: 1.6331, acc: 0.9893, lr: 0.00001375. 
2025-07-12 15:58:57,386 - root - INFO - Epoch 16, loss: 1.6195, acc: 0.9860, lr: 0.00001257. 
2025-07-12 15:59:16,803 - root - INFO - Epoch 17, loss: 1.6068, acc: 0.9840, lr: 0.00001141. 
2025-07-12 15:59:36,229 - root - INFO - Epoch 18, loss: 1.5924, acc: 0.9853, lr: 0.00001027. 
2025-07-12 15:59:55,855 - root - INFO - Epoch 19, loss: 1.5833, acc: 0.9820, lr: 0.00000917. 
2025-07-12 16:00:15,150 - root - INFO - Epoch 20, loss: 1.5670, acc: 0.9900, lr: 0.00000813. 
2025-07-12 16:00:34,384 - root - INFO - Epoch 21, loss: 1.5548, acc: 0.9933, lr: 0.00000714. 
2025-07-12 16:00:53,686 - root - INFO - Epoch 22, loss: 1.5468, acc: 0.9940, lr: 0.00000622. 
2025-07-12 16:01:13,022 - root - INFO - Epoch 23, loss: 1.5377, acc: 0.9973, lr: 0.00000539. 
2025-07-12 16:01:32,405 - root - INFO - Epoch 24, loss: 1.5313, acc: 0.9980, lr: 0.00000465. 
2025-07-12 16:01:51,656 - root - INFO - Epoch 25, loss: 1.5262, acc: 0.9980, lr: 0.00000401. 
2025-07-12 16:02:10,988 - root - INFO - Epoch 26, loss: 1.5215, acc: 0.9980, lr: 0.00000347. 
2025-07-12 16:02:30,333 - root - INFO - Epoch 27, loss: 1.5177, acc: 0.9980, lr: 0.00000305. 
2025-07-12 16:02:49,582 - root - INFO - Epoch 28, loss: 1.5142, acc: 0.9980, lr: 0.00000275. 
2025-07-12 16:03:09,023 - root - INFO - Epoch 29, loss: 1.5110, acc: 0.9980, lr: 0.00000256. 
2025-07-12 16:03:28,367 - root - INFO - Epoch 30, loss: 1.5072, acc: 0.9993, lr: 0.00000250. 
2025-07-12 16:03:47,747 - root - INFO - Epoch 31, loss: 1.5042, acc: 0.9993, lr: 0.00000256. 
2025-07-12 16:04:07,083 - root - INFO - Epoch 32, loss: 1.5007, acc: 1.0000, lr: 0.00000275. 
2025-07-12 16:04:26,441 - root - INFO - Epoch 33, loss: 1.4973, acc: 1.0000, lr: 0.00000305. 
2025-07-12 16:04:45,759 - root - INFO - Epoch 34, loss: 1.4935, acc: 1.0000, lr: 0.00000347. 
2025-07-12 16:05:05,090 - root - INFO - Epoch 35, loss: 1.4891, acc: 1.0000, lr: 0.00000401. 
2025-07-12 16:05:24,495 - root - INFO - Epoch 36, loss: 1.4840, acc: 1.0000, lr: 0.00000465. 
2025-07-12 16:05:43,815 - root - INFO - Epoch 37, loss: 1.4783, acc: 1.0000, lr: 0.00000539. 
2025-07-12 16:06:03,064 - root - INFO - Epoch 38, loss: 1.4713, acc: 1.0000, lr: 0.00000622. 
2025-07-12 16:06:22,383 - root - INFO - Epoch 39, loss: 1.4637, acc: 1.0000, lr: 0.00000714. 
2025-07-12 16:06:41,756 - root - INFO - Epoch 40, loss: 1.4548, acc: 1.0000, lr: 0.00000813. 
2025-07-12 16:07:01,075 - root - INFO - Epoch 41, loss: 1.4450, acc: 1.0000, lr: 0.00000917. 
2025-07-12 16:07:20,409 - root - INFO - Epoch 42, loss: 1.4338, acc: 1.0000, lr: 0.00001027. 
2025-07-12 16:07:39,718 - root - INFO - Epoch 43, loss: 1.4216, acc: 1.0000, lr: 0.00001141. 
2025-07-12 16:07:59,037 - root - INFO - Epoch 44, loss: 1.4080, acc: 1.0000, lr: 0.00001257. 
2025-07-12 16:08:18,389 - root - INFO - Epoch 45, loss: 1.3934, acc: 1.0000, lr: 0.00001375. 
2025-07-12 16:08:37,743 - root - INFO - Epoch 46, loss: 1.3775, acc: 1.0000, lr: 0.00001493. 
2025-07-12 16:08:57,200 - root - INFO - Epoch 47, loss: 1.3606, acc: 1.0000, lr: 0.00001609. 
2025-07-12 16:09:16,579 - root - INFO - Epoch 48, loss: 1.3425, acc: 1.0000, lr: 0.00001723. 
2025-07-12 16:09:35,949 - root - INFO - Epoch 49, loss: 1.3236, acc: 1.0000, lr: 0.00001833. 
2025-07-12 16:09:55,217 - root - INFO - Epoch 50, loss: 1.3036, acc: 1.0000, lr: 0.00001938. 
2025-07-12 16:10:14,539 - root - INFO - Epoch 51, loss: 1.2829, acc: 1.0000, lr: 0.00002036. 
2025-07-12 16:10:33,948 - root - INFO - Epoch 52, loss: 1.2615, acc: 1.0000, lr: 0.00002128. 
2025-07-12 16:10:53,355 - root - INFO - Epoch 53, loss: 1.2396, acc: 1.0000, lr: 0.00002211. 
2025-07-12 16:11:12,687 - root - INFO - Epoch 54, loss: 1.2174, acc: 1.0000, lr: 0.00002285. 
2025-07-12 16:11:31,892 - root - INFO - Epoch 55, loss: 1.1947, acc: 1.0000, lr: 0.00002349. 
2025-07-12 16:11:51,303 - root - INFO - Epoch 56, loss: 1.1721, acc: 1.0000, lr: 0.00002403. 
2025-07-12 16:12:10,605 - root - INFO - Epoch 57, loss: 1.1493, acc: 1.0000, lr: 0.00002445. 
2025-07-12 16:12:29,949 - root - INFO - Epoch 58, loss: 1.1267, acc: 1.0000, lr: 0.00002475. 
2025-07-12 16:12:49,312 - root - INFO - Epoch 59, loss: 1.1043, acc: 1.0000, lr: 0.00002494. 
2025-07-12 16:12:49,332 - root - INFO - Checkpoint list, length: 4: 
2025-07-12 16:12:49,332 - root - INFO - 	ckp_1 
2025-07-12 16:12:49,332 - root - INFO - 	ckp_2 
2025-07-12 16:12:49,332 - root - INFO - 	ckp_3 
2025-07-12 16:12:49,332 - root - INFO - 	ckp_4 
2025-07-12 16:12:49,345 - root - INFO - Training on checkpoint ckp_1. 
2025-07-12 16:12:49,345 - root - INFO - Subset length: 0
2025-07-12 16:12:49,345 - root - INFO - Gathering samples from checkpoints: ['ckp_1']
2025-07-12 16:12:49,346 - root - INFO - Subset length: 46
2025-07-12 16:12:49,346 - root - INFO - Training dataset size: 0. 
2025-07-12 16:12:49,346 - root - INFO - Evaluation dataset size: 46. 
2025-07-12 16:12:49,346 - root - INFO - OOD mask: 0 / 0. 
2025-07-12 16:12:49,346 - root - INFO - AL mask: 0 / 0. 
2025-07-12 16:12:49,346 - root - INFO - Evaluating on next checkpoint ckp_1. 
2025-07-12 16:12:50,335 - root - INFO - Number of samples: 46, acc: 0.9565, balanced acc: 0.9545, loss: 1.6042. 
2025-07-12 16:12:50,336 - root - INFO - Saving predictions to /fs/scratch/PAS2099/mino/ICICLE/log_auto/pipeline/serengeti_serengeti_F11/upper_bound/2025-07-12-15-53-11/bioclip2/full_text_head/2025-07-12-15-53-19/log. 
2025-07-12 16:12:50,342 - root - INFO - Training on checkpoint ckp_2. 
2025-07-12 16:12:50,342 - root - INFO - Gathering samples from checkpoints: ['ckp_1']
2025-07-12 16:12:50,345 - root - INFO - Subset length: 131
2025-07-12 16:12:50,345 - root - INFO - Gathering samples from checkpoints: ['ckp_2']
2025-07-12 16:12:50,346 - root - INFO - Subset length: 53
2025-07-12 16:12:50,347 - root - INFO - Training dataset size: 131. 
2025-07-12 16:12:50,347 - root - INFO - Evaluation dataset size: 53. 
2025-07-12 16:12:50,347 - root - INFO - OOD mask: 0 / 131. 
2025-07-12 16:12:50,347 - root - INFO - AL mask: 0 / 131. 
2025-07-12 16:12:50,347 - root - INFO - Evaluating on current checkpoint ckp_1. 
2025-07-12 16:12:50,347 - root - INFO - Gathering samples from checkpoints: ['ckp_1']
2025-07-12 16:12:50,348 - root - INFO - Subset length: 46
2025-07-12 16:12:50,883 - root - INFO - Eval on current training checkpoint ckp_1: Number of samples: 46, acc: 0.9565, balanced acc: 0.9545, loss: 1.6042. 
2025-07-12 16:12:50,883 - root - INFO - Evaluating on next checkpoint ckp_2. 
2025-07-12 16:12:51,886 - root - INFO - Number of samples: 53, acc: 0.6604, balanced acc: 0.6667, loss: 1.7361. 
2025-07-12 16:12:51,887 - root - INFO - Saving predictions to /fs/scratch/PAS2099/mino/ICICLE/log_auto/pipeline/serengeti_serengeti_F11/upper_bound/2025-07-12-15-53-11/bioclip2/full_text_head/2025-07-12-15-53-19/log. 
2025-07-12 16:12:51,889 - root - INFO - Training on checkpoint ckp_3. 
2025-07-12 16:12:51,889 - root - INFO - Gathering samples from checkpoints: ['ckp_2']
2025-07-12 16:12:51,892 - root - INFO - Subset length: 129
2025-07-12 16:12:51,893 - root - INFO - Gathering samples from checkpoints: ['ckp_3']
2025-07-12 16:12:51,894 - root - INFO - Subset length: 66
2025-07-12 16:12:51,894 - root - INFO - Training dataset size: 129. 
2025-07-12 16:12:51,894 - root - INFO - Evaluation dataset size: 66. 
2025-07-12 16:12:51,894 - root - INFO - OOD mask: 0 / 129. 
2025-07-12 16:12:51,894 - root - INFO - AL mask: 0 / 129. 
2025-07-12 16:12:51,894 - root - INFO - Evaluating on current checkpoint ckp_2. 
2025-07-12 16:12:51,895 - root - INFO - Gathering samples from checkpoints: ['ckp_2']
2025-07-12 16:12:51,896 - root - INFO - Subset length: 53
2025-07-12 16:12:52,476 - root - INFO - Eval on current training checkpoint ckp_2: Number of samples: 53, acc: 0.6604, balanced acc: 0.6667, loss: 1.7361. 
2025-07-12 16:12:52,477 - root - INFO - Evaluating on next checkpoint ckp_3. 
2025-07-12 16:12:54,205 - root - INFO - Number of samples: 66, acc: 0.3030, balanced acc: 0.2917, loss: 2.2772. 
2025-07-12 16:12:54,205 - root - INFO - Saving predictions to /fs/scratch/PAS2099/mino/ICICLE/log_auto/pipeline/serengeti_serengeti_F11/upper_bound/2025-07-12-15-53-11/bioclip2/full_text_head/2025-07-12-15-53-19/log. 
2025-07-12 16:12:54,206 - root - INFO - Training on checkpoint ckp_4. 
2025-07-12 16:12:54,206 - root - INFO - Gathering samples from checkpoints: ['ckp_3']
2025-07-12 16:12:54,226 - root - INFO - Subset length: 946
2025-07-12 16:12:54,226 - root - INFO - Gathering samples from checkpoints: ['ckp_4']
2025-07-12 16:12:54,227 - root - INFO - Subset length: 44
2025-07-12 16:12:54,227 - root - INFO - Training dataset size: 946. 
2025-07-12 16:12:54,227 - root - INFO - Evaluation dataset size: 44. 
2025-07-12 16:12:54,227 - root - INFO - OOD mask: 0 / 946. 
2025-07-12 16:12:54,227 - root - INFO - AL mask: 0 / 946. 
2025-07-12 16:12:54,227 - root - INFO - Evaluating on current checkpoint ckp_3. 
2025-07-12 16:12:54,227 - root - INFO - Gathering samples from checkpoints: ['ckp_3']
2025-07-12 16:12:54,229 - root - INFO - Subset length: 66
2025-07-12 16:12:55,209 - root - INFO - Eval on current training checkpoint ckp_3: Number of samples: 66, acc: 0.3030, balanced acc: 0.2917, loss: 2.2772. 
2025-07-12 16:12:55,210 - root - INFO - Evaluating on next checkpoint ckp_4. 
2025-07-12 16:12:56,559 - root - INFO - Number of samples: 44, acc: 0.6136, balanced acc: 0.6042, loss: 1.8716. 
2025-07-12 16:12:56,559 - root - INFO - Saving predictions to /fs/scratch/PAS2099/mino/ICICLE/log_auto/pipeline/serengeti_serengeti_F11/upper_bound/2025-07-12-15-53-11/bioclip2/full_text_head/2025-07-12-15-53-19/log. 
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:          accuracy ‚ñà‚ñÖ‚ñÅ‚ñÑ
wandb: balanced_accuracy ‚ñà‚ñÖ‚ñÅ‚ñÑ
wandb: 
wandb: Run summary:
wandb:          accuracy 0.61364
wandb: balanced_accuracy 0.60417
wandb: 
wandb: üöÄ View run serengeti_serengeti_F11 | upper_bound | 2025-07-12-15-53-11 | bioclip2 | full_text_head | 2025-07-12-15-53-19 at: https://wandb.ai/minodaze-the-ohio-state-university/ICICLE-Benchmark/runs/9eqtcgl1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/minodaze-the-ohio-state-university/ICICLE-Benchmark
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250712_155319-9eqtcgl1/logs
2025-07-12 16:12:57,104 - root - INFO - Elapsed time: 1177.85 seconds. 
/users/PAS2099/mino/miniconda3/envs/ICICLE/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
2025-07-12 16:13:04,369 - root - INFO - Saving to /fs/scratch/PAS2099/mino/ICICLE/log_auto/pipeline/serengeti_serengeti_C01/upper_bound/2025-07-12-16-12-59/bioclip2/full_text_head/2025-07-12-16-13-04/log. 
wandb: Currently logged in as: minodaze (minodaze-the-ohio-state-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.20.1
wandb: Run data is saved locally in /users/PAS2099/mino/ICICLE/wandb/run-20250712_161304-iuspbtwb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serengeti_serengeti_C01 | upper_bound | 2025-07-12-16-12-59 | bioclip2 | full_text_head | 2025-07-12-16-13-04
wandb: ‚≠êÔ∏è View project at https://wandb.ai/minodaze-the-ohio-state-university/ICICLE-Benchmark
wandb: üöÄ View run at https://wandb.ai/minodaze-the-ohio-state-university/ICICLE-Benchmark/runs/iuspbtwb
2025-07-12 16:13:05,380 - root - INFO - wandb logging is enabled.
2025-07-12 16:13:05,381 - root - INFO - {'accu_eval': False,
 'adapter_bottleneck': 64,
 'adapter_init': 'lora_kaiming',
 'adapter_scaler': 0.1,
 'al_config': {'method': 'none'},
 'attention_index': None,
 'attention_type': 'full',
 'bitfit': False,
 'block_index': None,
 'c': '/fs/scratch/PAS2099/mino/ICICLE/configs/generated_common/serengeti_serengeti_C01_upper_bound.yaml',
 'cl_config': {'method': 'none'},
 'common_config': {'model': 'bioclip2', 'train_data_config_path': '/fs/scratch/PAS2099/camera-trap-benchmark/serengeti/serengeti_C01/30/train.json', 'eval_data_config_path': '/fs/scratch/PAS2099/camera-trap-benchmark/serengeti/serengeti_C01/30/test.json', 'all_data_config_path': '/fs/scratch/PAS2099/camera-trap-benchmark/serengeti/serengeti_C01/30/train-all.json', 'train_batch_size': 32, 'eval_batch_size': 512, 'optimizer_name': 'AdamW', 'optimizer_params': {'lr': 2.5e-05, 'weight_decay': 0.0001}, 'chop_head': False, 'scheduler': 'CosineAnnealingLR', 'scheduler_params': {'T_max': 30, 'eta_min': 2.5e-06}},
 'convpass_bottleneck': 8,
 'convpass_init': 'lora_xavier',
 'convpass_scaler': 10,
 'convpass_xavier_init': False,
 'debug': False,
 'device': 'cuda',
 'difffit': False,
 'drop_path_rate': 0.0,
 'eval_only': False,
 'eval_per_epoch': False,
 'fact_dim': 8,
 'fact_scaler': 1.0,
 'fact_type': None,
 'ft_attn_ln': 'before',
 'ft_attn_mode': 'parallel',
 'ft_attn_module': None,
 'ft_mlp_ln': 'before',
 'ft_mlp_mode': 'parallel',
 'ft_mlp_module': None,
 'full': True,
 'generalization_test': 'a',
 'gpu_id': None,
 'gpu_memory_monitor': False,
 'interpolation_alpha': 0.5,
 'interpolation_head': False,
 'interpolation_model': False,
 'is_save': False,
 'label_type': 'common',
 'ln': False,
 'log_path': '/fs/scratch/PAS2099/mino/ICICLE/log_auto/pipeline/serengeti_serengeti_C01/upper_bound/2025-07-12-16-12-59/',
 'lora_bottleneck': 0,
 'merge_factor': 1,
 'mlp_index': None,
 'mlp_type': 'full',
 'module_name': 'upper_bound',
 'no_gpu_monitor_colors': False,
 'ood_config': {'method': 'none'},
 'pretrain_config': {'pretrain': True, 'pretrain_data_config_path': '/fs/scratch/PAS2099/camera-trap-benchmark/serengeti/serengeti_C01/30/train-all.json', 'epochs': 60, 'loss_type': 'ce'},
 'pretrained_weights': 'bioclip2',
 'repadapter_bottleneck': 8,
 'repadapter_group': 2,
 'repadapter_init': 'lora_xavier',
 'repadapter_scaler': 1,
 'save_dir': '/fs/scratch/PAS2099/mino/ICICLE/log_auto/pipeline/serengeti_serengeti_C01/upper_bound/2025-07-12-16-12-59/bioclip2/full_text_head/2025-07-12-16-13-04/log',
 'seed': 9527,
 'ssf': False,
 'text': 'head',
 'text_template': 'openai',
 'vpt_dropout': 0.1,
 'vpt_layer': None,
 'vpt_mode': None,
 'vpt_num': 10,
 'vqt_dropout': 0.1,
 'vqt_num': 0,
 'wandb': True}
2025-07-12 16:13:05,422 - root - INFO - Using Bioclip-2 model. 
2025-07-12 16:13:05,422 - root - INFO - Loaded ViT-L-14 model config.
2025-07-12 16:13:07,334 - root - INFO - Loading pretrained ViT-L-14 weights (pretrained_weights/bioclip-2/open_clip_pytorch_model.bin).
2025-07-12 16:13:10,505 - root - INFO - Pretraining classifier... 
2025-07-12 16:13:10,769 - root - INFO - Gathering samples from checkpoints: ['ckp_-1', 'ckp_1']
2025-07-12 16:13:11,257 - root - INFO - Checkpoint ckp_1 not found
2025-07-12 16:13:11,257 - root - INFO - Subset length: 18886
2025-07-12 16:13:11,268 - root - INFO - Pretrain dataset size: 18886. 
